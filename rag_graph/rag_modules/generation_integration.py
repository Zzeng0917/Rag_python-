"""
ç”Ÿæˆé›†æˆæ¨¡å—
è´Ÿè´£LLMé›†æˆå’Œæ—…æ¸¸é—®ç­”ç”Ÿæˆ
"""

import logging
import os
import time
from typing import List, Iterator, Optional
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

logger = logging.getLogger(__name__)

class GenerationIntegrationModule:
    """ç”Ÿæˆé›†æˆæ¨¡å— - è´Ÿè´£ç­”æ¡ˆç”Ÿæˆ"""

    def __init__(self, config=None, model_name: str = "gpt-3.5-turbo", temperature: float = 0.1, max_tokens: int = 2048):
        """
        åˆå§‹åŒ–ç”Ÿæˆé›†æˆæ¨¡å—

        Args:
            config: é…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            model_name: æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.config = config
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.client = None

        # åˆå§‹åŒ–LLMæ¨¡å‹
        self.setup_llm()

    def setup_llm(self):
        """åˆå§‹åŒ–LLMæ¨¡å‹"""
        logger.info("æ­£åœ¨åˆå§‹åŒ–LLMæ¨¡å‹")

        # ä¼˜å…ˆä»configè·å–é…ç½®ï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è·å–
        if self.config and hasattr(self.config, 'llm_api_key') and self.config.llm_api_key:
            api_key = self.config.llm_api_key
            base_url = self.config.llm_base_url
            self.model_name = self.config.llm_model
        else:
            # ä»ç¯å¢ƒå˜é‡è·å–ï¼ˆæ”¯æŒå¤šç§å˜é‡åï¼‰
            api_key = (os.getenv("LLM_API_KEY") or 
                      os.getenv("ZHIPU_API_KEY") or 
                      os.getenv("GLM_API_KEY") or 
                      os.getenv("OPENAI_API_KEY"))
            base_url = os.getenv("LLM_BASE_URL")
            model_name = os.getenv("LLM_MODEL")
            if model_name:
                self.model_name = model_name

        if not api_key:
            raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: LLM_API_KEY æˆ– ZHIPU_API_KEY")

        try:
            # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆæœ¬åœ°æ¨¡å‹éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
            timeout = float(os.getenv("REQUEST_TIMEOUT", "60.0"))

            if base_url:
                self.client = OpenAI(
                    api_key=api_key,
                    base_url=base_url,
                    timeout=timeout
                )
                logger.info(f"LLMæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè¶…æ—¶æ—¶é—´: {timeout}ç§’")
            else:
                self.client = OpenAI(
                    api_key=api_key,
                    timeout=timeout
                )
                logger.info(f"OpenAIæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè¶…æ—¶æ—¶é—´: {timeout}ç§’")
        except Exception as e:
            logger.error(f"LLMæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def generate_adaptive_answer(self, question: str, documents: List[Document]) -> str:
        """
        æ™ºèƒ½ç»Ÿä¸€ç­”æ¡ˆç”Ÿæˆ
        è‡ªåŠ¨é€‚åº”ä¸åŒç±»å‹çš„æŸ¥è¯¢ï¼Œæ— éœ€é¢„å…ˆåˆ†ç±»

        Args:
            question: ç”¨æˆ·é—®é¢˜
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []

        for doc in documents:
            content = doc.page_content.strip()
            if content:
                # æ·»åŠ æ£€ç´¢å±‚çº§ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                level = doc.metadata.get('retrieval_level', '')
                if level:
                    context_parts.append(f"[{level.upper()}] {content}")
                else:
                    context_parts.append(content)

        context = "\n".join(context_parts)

        # æ™ºèƒ½æç¤ºè¯æ¨¡æ¿
        prompt = f"""
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸é¡¾é—®ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ—…æ¸¸ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

        æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜ï¼š{question}

        è¯·æä¾›å‡†ç¡®ã€å®ç”¨çš„å›ç­”ã€‚æ ¹æ®é—®é¢˜çš„æ€§è´¨ï¼š
        - å¦‚æœæ˜¯è¯¢é—®å¤šä¸ªæ™¯ç‚¹ï¼Œè¯·æä¾›æ¸…æ™°çš„åˆ—è¡¨
        - å¦‚æœæ˜¯è¯¢é—®å…·ä½“ä¿¡æ¯ï¼ˆå¦‚é—¨ç¥¨ã€å¼€æ”¾æ—¶é—´ï¼‰ï¼Œè¯·æä¾›è¯¦ç»†ä¿¡æ¯
        - å¦‚æœæ˜¯è¯¢é—®äº¤é€šæˆ–ä½å®¿ï¼Œè¯·æä¾›å®ç”¨å»ºè®®
        - å¦‚æœæ˜¯ä¸€èˆ¬æ€§å’¨è¯¢ï¼Œè¯·æä¾›ç»¼åˆæ€§å›ç­”

        å›ç­”ï¼š
        """

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            logger.error(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"

    def generate_adaptive_answer_stream(self, question: str, documents: List[Document], max_retries: int = 3) -> Iterator[str]:
        """
        æ™ºèƒ½ç»Ÿä¸€æµå¼ç­”æ¡ˆç”Ÿæˆï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            question: ç”¨æˆ·é—®é¢˜
            documents: æ–‡æ¡£åˆ—è¡¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Yields:
            å›ç­”ç‰‡æ®µ
        """
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []

        for doc in documents:
            content = doc.page_content.strip()
            if content:
                # æ·»åŠ æ£€ç´¢å±‚çº§ä¿¡æ¯
                level = doc.metadata.get('retrieval_level', '')
                if level:
                    context_parts.append(f"[{level.upper()}] {content}")
                else:
                    context_parts.append(content)

        context = "\n".join(context_parts)

        # æ™ºèƒ½æç¤ºè¯æ¨¡æ¿
        prompt = f"""
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

        æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜ï¼š{question}

        è¯·æä¾›å‡†ç¡®ã€å®ç”¨çš„å›ç­”ã€‚æ ¹æ®é—®é¢˜çš„æ€§è´¨ï¼š
        - å¦‚æœæ˜¯è¯¢é—®å¤šä¸ªæ™¯ç‚¹ï¼Œè¯·æä¾›æ¸…æ™°çš„åˆ—è¡¨
        - å¦‚æœæ˜¯è¯¢é—®å…·ä½“ä¿¡æ¯ï¼ˆå¦‚é—¨ç¥¨ã€å¼€æ”¾æ—¶é—´ï¼‰ï¼Œè¯·æä¾›è¯¦ç»†ä¿¡æ¯
        - å¦‚æœæ˜¯è¯¢é—®äº¤é€šæˆ–ä½å®¿ï¼Œè¯·æä¾›å®ç”¨å»ºè®®
        - å¦‚æœæ˜¯ä¸€èˆ¬æ€§å’¨è¯¢ï¼Œè¯·æä¾›ç»¼åˆæ€§å›ç­”

        å›ç­”ï¼š
        """

        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    stream=True,
                    timeout=60  # æ·»åŠ è¶…æ—¶è®¾ç½®
                )

                if attempt == 0:
                    print("å¼€å§‹æµå¼å›ç­”ç”Ÿæˆ...")
                else:
                    print(f"ç¬¬{attempt + 1}æ¬¡å°è¯•æµå¼ç”Ÿæˆ...")

                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield content

                # å¦‚æœæˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                return

            except Exception as e:
                logger.warning(f"æµå¼ç”Ÿæˆç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {e}")

                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2  # é€’å¢ç­‰å¾…æ—¶é—´
                    print(f"âš ï¸ è¿æ¥ä¸­æ–­ï¼Œ{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨éæµå¼ä½œä¸ºåå¤‡
                    logger.error("æµå¼ç”Ÿæˆå®Œå…¨å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æ ‡å‡†æ¨¡å¼...")

                    try:
                        fallback_response = self.generate_adaptive_answer(question, documents)
                        yield fallback_response
                        return
                    except Exception as fallback_error:
                        logger.error(f"åå¤‡ç”Ÿæˆä¹Ÿå¤±è´¥: {fallback_error}")
                        error_msg = f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°ç½‘ç»œé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(fallback_error)}"
                        yield error_msg
                        return

    def query_rewrite(self, query: str) -> str:
        """
        æ™ºèƒ½æŸ¥è¯¢é‡å†™ - è®©å¤§æ¨¡å‹åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™æŸ¥è¯¢ä»¥æé«˜æ—…æ¸¸ä¿¡æ¯æœç´¢æ•ˆæœ

        Args:
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            é‡å†™åçš„æŸ¥è¯¢æˆ–åŸå§‹æŸ¥è¯¢
        """
        prompt = PromptTemplate(
            template="""
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æŸ¥è¯¢åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™ä»¥æé«˜æ—…æ¸¸ä¿¡æ¯æœç´¢æ•ˆæœã€‚

        åŸå§‹æŸ¥è¯¢ï¼š{query}

        åˆ†æè§„åˆ™ï¼š
        1. **å…·ä½“æ˜ç¡®çš„æŸ¥è¯¢**ï¼ˆç›´æ¥è¿”å›åŸæŸ¥è¯¢ï¼‰ï¼š
            - åŒ…å«å…·ä½“æ™¯ç‚¹åç§°ï¼šå¦‚"æ•…å®«æ€ä¹ˆå»"ã€"é•¿åŸé—¨ç¥¨ä»·æ ¼"
            - æ˜ç¡®çš„æ—…æ¸¸è¯¢é—®ï¼šå¦‚"åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„"ã€"ä¸Šæµ·è¿ªå£«å°¼æ”»ç•¥"
            - å…·ä½“çš„äº¤é€šä½å®¿ï¼šå¦‚"æœºåœºåˆ°å¸‚åŒºæ€ä¹ˆèµ°"ã€"é…’åº—æ¨è"

        2. **æ¨¡ç³Šä¸æ¸…çš„æŸ¥è¯¢**ï¼ˆéœ€è¦é‡å†™ï¼‰ï¼š
            - è¿‡äºå®½æ³›ï¼šå¦‚"æ—…æ¸¸"ã€"å»å“ªç©"ã€"æ¨èä¸ªåœ°æ–¹"
            - ç¼ºä¹å…·ä½“ä¿¡æ¯ï¼šå¦‚"å›½å†…"ã€"å›½å¤–"ã€"ä¾¿å®œçš„"
            - å£è¯­åŒ–è¡¨è¾¾ï¼šå¦‚"æƒ³å»ç©"ã€"æœ‰ä»€ä¹ˆå¥½å»å¤„"

        é‡å†™åŸåˆ™ï¼š
        - ä¿æŒåŸæŸ¥è¯¢æ„å›¾ä¸å˜
        - å¢åŠ ç›¸å…³æ—…æ¸¸æœ¯è¯­
        - ä¼˜å…ˆæ¨èçƒ­é—¨æ™¯ç‚¹
        - ä¿æŒç®€æ´æ€§

        ç¤ºä¾‹ï¼š
        - "æ—…æ¸¸" â†’ "çƒ­é—¨æ—…æ¸¸æ™¯ç‚¹æ¨è"
        - "å»å“ªç©" â†’ "å‘¨æœ«æ—…æ¸¸æ™¯ç‚¹æ¨è"
        - "æ¨èä¸ªåœ°æ–¹" â†’ "å›½å†…çƒ­é—¨æ—…æ¸¸ç›®çš„åœ°"
        - "å›½å†…" â†’ "å›½å†…ç»å…¸æ—…æ¸¸è·¯çº¿"
        - "æ•…å®«æ€ä¹ˆå»" â†’ "æ•…å®«æ€ä¹ˆå»"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰
        - "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„" â†’ "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰

        è¯·åªè¾“å‡ºæœ€ç»ˆæŸ¥è¯¢ï¼ˆå¦‚æœä¸éœ€è¦é‡å†™å°±è¿”å›åŸæŸ¥è¯¢ï¼‰ï¼š
        """,
            input_variables=["query"]
        )

        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.client
            | StrOutputParser()
        )

        try:
            response = chain.invoke(query).strip()
            # è®°å½•é‡å†™ç»“æœ
            if response != query:
                logger.info(f"æŸ¥è¯¢å·²é‡å†™: '{query}' â†’ '{response}'")
            else:
                logger.info(f"æŸ¥è¯¢æ— éœ€é‡å†™: '{query}'")
            return response
        except Exception as e:
            logger.error(f"æŸ¥è¯¢é‡å†™å¤±è´¥: {e}")
            return query

    def query_router(self, query: str) -> str:
        """
        æŸ¥è¯¢è·¯ç”± - æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            è·¯ç”±ç±»å‹ ('list', 'detail', 'general')
        """
        prompt = ChatPromptTemplate.from_template("""
        æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œå°†å…¶åˆ†ç±»ä¸ºä»¥ä¸‹ä¸‰ç§ç±»å‹ä¹‹ä¸€ï¼š

        1. 'list' - ç”¨æˆ·æƒ³è¦è·å–æ™¯ç‚¹åˆ—è¡¨æˆ–æ¨èï¼Œåªéœ€è¦æ™¯ç‚¹åç§°
            ä¾‹å¦‚ï¼šæ¨èå‡ ä¸ªæ™¯ç‚¹ã€åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„ã€ç»™æˆ‘3ä¸ªå¿…å»çš„åœ°æ–¹

        2. 'detail' - ç”¨æˆ·æƒ³è¦å…·ä½“çš„æ—…æ¸¸ä¿¡æ¯æˆ–è¯¦ç»†æŒ‡å—
            ä¾‹å¦‚ï¼šæ•…å®«æ€ä¹ˆå»ã€é—¨ç¥¨å¤šå°‘é’±ã€å¼€æ”¾æ—¶é—´ã€æ—…æ¸¸æ”»ç•¥

        3. 'general' - å…¶ä»–ä¸€èˆ¬æ€§é—®é¢˜
            ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æ–‡åŒ–æ—…æ¸¸ã€æ—…æ¸¸æ³¨æ„äº‹é¡¹ã€æœ€ä½³æ—…æ¸¸å­£èŠ‚

        è¯·åªè¿”å›åˆ†ç±»ç»“æœï¼šlistã€detail æˆ– general

        ç”¨æˆ·é—®é¢˜ï¼š{query}

        åˆ†ç±»ç»“æœï¼š
        """)

        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.client
            | StrOutputParser()
        )

        try:
            result = chain.invoke(query).strip().lower()
            # ç¡®ä¿è¿”å›æœ‰æ•ˆçš„è·¯ç”±ç±»å‹
            if result in ['list', 'detail', 'general']:
                return result
            else:
                return 'general'  # é»˜è®¤ç±»å‹
        except Exception as e:
            logger.error(f"æŸ¥è¯¢è·¯ç”±å¤±è´¥: {e}")
            return 'general'  # é»˜è®¤ç±»å‹

    def generate_list_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”Ÿæˆåˆ—è¡¨å¼å›ç­” - é€‚ç”¨äºæ¨èç±»æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            åˆ—è¡¨å¼å›ç­”
        """
        if not context_docs:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ—…æ¸¸æ™¯ç‚¹ä¿¡æ¯ã€‚"

        # æå–åœ°ç‚¹åç§°
        location_names = []
        for doc in context_docs:
            location_name = doc.metadata.get('location_name',
                           doc.metadata.get('name',
                           doc.metadata.get('entity_name', 'æœªçŸ¥åœ°ç‚¹')))
            if location_name and location_name not in location_names:
                location_names.append(location_name)

        # æ„å»ºç®€æ´çš„åˆ—è¡¨å›ç­”
        if len(location_names) == 1:
            return f"ä¸ºæ‚¨æ¨èï¼š{location_names[0]}"
        elif len(location_names) <= 3:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹æ™¯ç‚¹ï¼š\n" + "\n".join([f"{i+1}. {name}" for i, name in enumerate(location_names)])
        else:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹æ™¯ç‚¹ï¼š\n" + "\n".join([f"{i+1}. {name}" for i, name in enumerate(location_names[:3])]) + f"\n\nè¿˜æœ‰å…¶ä»–{len(location_names)-3}ä¸ªæ™¯ç‚¹å¯ä¾›é€‰æ‹©ã€‚"

    def generate_basic_answer_stream(self, query: str, context_docs: List[Document]) -> Iterator[str]:
        """
        ç”ŸæˆåŸºç¡€å›ç­” - æµå¼è¾“å‡º

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Yields:
            å›ç­”ç‰‡æ®µ
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸é¡¾é—®ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ—…æ¸¸ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

        ç”¨æˆ·é—®é¢˜ï¼š{query}
        ç›¸å…³æ—…æ¸¸ä¿¡æ¯ï¼š
        {context}
        è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

        å›ç­”ï¼š
        """)

        chain = (
            {"query": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.client
            | StrOutputParser()
        )

        try:
            for chunk in chain.stream(query):
                yield chunk
        except Exception as e:
            logger.error(f"æµå¼å›ç­”ç”Ÿæˆå¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"

    def generate_detailed_guide_answer_stream(self, query: str, context_docs: List[Document]) -> Iterator[str]:
        """
        ç”Ÿæˆè¯¦ç»†æ—…æ¸¸æŒ‡å—å›ç­” - æµå¼è¾“å‡º

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Yields:
            è¯¦ç»†æ—…æ¸¸æŒ‡å—å›ç­”ç‰‡æ®µ
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸è§„åˆ’å¸ˆã€‚è¯·æ ¹æ®æ—…æ¸¸ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„æ—…æ¸¸æŒ‡å—ã€‚

        ç”¨æˆ·é—®é¢˜ï¼š{query}
        ç›¸å…³æ—…æ¸¸ä¿¡æ¯ï¼š
        {context}
        è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

        ## ğŸ›ï¸ æ™¯ç‚¹ä»‹ç»
        [ç®€è¦ä»‹ç»æ™¯ç‚¹ç‰¹è‰²å’Œäº®ç‚¹]

        ## ğŸ“ åŸºæœ¬ä¿¡æ¯
        [åœ°å€ã€å¼€æ”¾æ—¶é—´ã€é—¨ç¥¨ä»·æ ¼ã€è”ç³»æ–¹å¼ç­‰]

        ## ğŸš— äº¤é€šæŒ‡å—
        [å¦‚ä½•åˆ°è¾¾ï¼ŒåŒ…æ‹¬å…¬å…±äº¤é€šå’Œè‡ªé©¾è·¯çº¿]

        ## ğŸ’¡ æ¸¸è§ˆå»ºè®®
        [æœ€ä½³æ¸¸è§ˆæ—¶é—´ã€æ¨èè·¯çº¿ã€æ³¨æ„äº‹é¡¹ç­‰]

        æ³¨æ„ï¼š
        - æ ¹æ®å®é™…å†…å®¹çµæ´»è°ƒæ•´ç»“æ„
        - ä¸è¦å¼ºè¡Œå¡«å……æ— å…³å†…å®¹
        - é‡ç‚¹çªå‡ºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§
        - å¦‚æœæ²¡æœ‰é¢å¤–çš„å»ºè®®è¦åˆ†äº«ï¼Œå¯ä»¥çœç•¥ç›¸åº”éƒ¨åˆ†

        å›ç­”ï¼š
        """)

        chain = (
            {"query": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.client
            | StrOutputParser()
        )

        try:
            for chunk in chain.stream(query):
                yield chunk
        except Exception as e:
            logger.error(f"è¯¦ç»†æŒ‡å—æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œç”Ÿæˆè¯¦ç»†æŒ‡å—æ—¶å‡ºç°é”™è¯¯ã€‚"

    def _build_context(self, docs: List[Document], max_length: int = 2000) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²

        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦

        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not docs:
            return "æš‚æ— ç›¸å…³æ—…æ¸¸ä¿¡æ¯ã€‚"

        context_parts = []
        current_length = 0

        for i, doc in enumerate(docs, 1):
            # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
            metadata_info = f"ã€æ—…æ¸¸ä¿¡æ¯ {i}ã€‘"

            # æå–å…³é”®å…ƒæ•°æ®
            node_type = doc.metadata.get('node_type', '')
            location_name = (doc.metadata.get('location_name') or
                           doc.metadata.get('name') or
                           doc.metadata.get('entity_name', ''))

            if location_name:
                metadata_info += f" {location_name}"

            if node_type:
                metadata_info += f" | ç±»å‹: {node_type}"

            # æ„å»ºæ–‡æ¡£æ–‡æœ¬
            doc_text = f"{metadata_info}\n{doc.page_content}"

            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(doc_text) > max_length:
                break

            context_parts.append(doc_text)
            current_length += len(doc_text)

        return "\n\n".join(context_parts)