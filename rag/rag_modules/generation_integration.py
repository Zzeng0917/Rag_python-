"""
ç”Ÿæˆé›†æˆæ¨¡å—
"""

import os
import logging
from typing import List

from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

logger = logging.getLogger(__name__)

class GenerationIntegrationModule:
    """æ—…æ¸¸ç”Ÿæˆé›†æˆæ¨¡å— - è´Ÿè´£LLMé›†æˆå’Œæ—…æ¸¸é—®ç­”ç”Ÿæˆ"""

    def __init__(self, config=None, temperature: float = 0.1, max_tokens: int = 2048):
        """
        åˆå§‹åŒ–ç”Ÿæˆé›†æˆæ¨¡å—

        Args:
            config: é…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.config = config
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.llm = None

        # åˆå§‹åŒ–GLM-4æ¨¡å‹
        self.setup_glm4()

    def setup_glm4(self):
        """åˆå§‹åŒ–GLM-4æ¨¡å‹"""
        logger.info("æ­£åœ¨åˆå§‹åŒ–GLM-4æ¨¡å‹")

        # è·å–APIå¯†é’¥
        api_key = os.getenv("GLM_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: GLM_API_KEY")

        # åˆ›å»ºGLM-4å®ä¾‹
        self.llm = ChatOpenAI(
            model="glm-4",
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            openai_api_key=api_key,
            openai_api_base="https://open.bigmodel.cn/api/paas/v4/"
        )

        logger.info("GLM-4æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def generate_basic_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”ŸæˆåŸºç¡€å›ç­”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸é¡¾é—®ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ—…æ¸¸ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³æ—…æ¸¸ä¿¡æ¯:
{context}

è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

å›ç­”:""")

        # ä½¿ç”¨LCELæ„å»ºé“¾
        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query)
        return response
    
    def generate_detailed_guide_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”Ÿæˆè¯¦ç»†æ—…æ¸¸æŒ‡å—å›ç­”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            è¯¦ç»†çš„æ—…æ¸¸æŒ‡å—å›ç­”
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸è§„åˆ’å¸ˆã€‚è¯·æ ¹æ®æ—…æ¸¸ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„æ—…æ¸¸æŒ‡å—ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³æ—…æ¸¸ä¿¡æ¯:
{context}

è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

## ğŸï¸ æ™¯ç‚¹ä»‹ç»
[ç®€è¦ä»‹ç»æ™¯ç‚¹ç‰¹è‰²å’Œäº®ç‚¹]

## ğŸ“ åŸºæœ¬ä¿¡æ¯
[åœ°å€ã€å¼€æ”¾æ—¶é—´ã€é—¨ç¥¨ä»·æ ¼ã€è”ç³»æ–¹å¼ç­‰]

## ğŸš— äº¤é€šæŒ‡å—
[å¦‚ä½•åˆ°è¾¾ï¼ŒåŒ…æ‹¬å…¬å…±äº¤é€šå’Œè‡ªé©¾è·¯çº¿]

## ğŸ’¡ æ¸¸è§ˆå»ºè®®
[æœ€ä½³æ¸¸è§ˆæ—¶é—´ã€æ¨èè·¯çº¿ã€æ³¨æ„äº‹é¡¹ç­‰]

æ³¨æ„ï¼š
- æ ¹æ®å®é™…å†…å®¹çµæ´»è°ƒæ•´ç»“æ„
- ä¸è¦å¼ºè¡Œå¡«å……æ— å…³å†…å®¹
- é‡ç‚¹çªå‡ºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§
- å¦‚æœæ²¡æœ‰é¢å¤–çš„å»ºè®®è¦åˆ†äº«ï¼Œå¯ä»¥çœç•¥ç›¸åº”éƒ¨åˆ†

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query)
        return response
    
    def query_rewrite(self, query: str) -> str:
        """
        æ™ºèƒ½æŸ¥è¯¢é‡å†™ - è®©å¤§æ¨¡å‹åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™æŸ¥è¯¢

        Args:
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            é‡å†™åçš„æŸ¥è¯¢æˆ–åŸæŸ¥è¯¢
        """
        prompt = PromptTemplate(
            template="""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æŸ¥è¯¢åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™ä»¥æé«˜æ—…æ¸¸ä¿¡æ¯æœç´¢æ•ˆæœã€‚

åŸå§‹æŸ¥è¯¢: {query}

åˆ†æè§„åˆ™ï¼š
1. **å…·ä½“æ˜ç¡®çš„æŸ¥è¯¢**ï¼ˆç›´æ¥è¿”å›åŸæŸ¥è¯¢ï¼‰ï¼š
   - åŒ…å«å…·ä½“æ™¯ç‚¹åç§°ï¼šå¦‚"æ•…å®«æ€ä¹ˆå»"ã€"é•¿åŸé—¨ç¥¨ä»·æ ¼"
   - æ˜ç¡®çš„æ—…æ¸¸è¯¢é—®ï¼šå¦‚"åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„"ã€"ä¸Šæµ·è¿ªå£«å°¼æ”»ç•¥"
   - å…·ä½“çš„äº¤é€šä½å®¿ï¼šå¦‚"æœºåœºåˆ°å¸‚åŒºæ€ä¹ˆèµ°"ã€"é…’åº—æ¨è"

2. **æ¨¡ç³Šä¸æ¸…çš„æŸ¥è¯¢**ï¼ˆéœ€è¦é‡å†™ï¼‰ï¼š
   - è¿‡äºå®½æ³›ï¼šå¦‚"æ—…æ¸¸"ã€"å»å“ªç©"ã€"æ¨èä¸ªåœ°æ–¹"
   - ç¼ºä¹å…·ä½“ä¿¡æ¯ï¼šå¦‚"å›½å†…"ã€"å›½å¤–"ã€"ä¾¿å®œçš„"
   - å£è¯­åŒ–è¡¨è¾¾ï¼šå¦‚"æƒ³å»ç©"ã€"æœ‰ä»€ä¹ˆå¥½å»å¤„"

é‡å†™åŸåˆ™ï¼š
- ä¿æŒåŸæ„ä¸å˜
- å¢åŠ ç›¸å…³æ—…æ¸¸æœ¯è¯­
- ä¼˜å…ˆæ¨èçƒ­é—¨æ™¯ç‚¹
- ä¿æŒç®€æ´æ€§

ç¤ºä¾‹ï¼š
- "æ—…æ¸¸" â†’ "çƒ­é—¨æ—…æ¸¸æ™¯ç‚¹æ¨è"
- "å»å“ªç©" â†’ "å‘¨æœ«æ—…æ¸¸æ™¯ç‚¹æ¨è"
- "æ¨èä¸ªåœ°æ–¹" â†’ "å›½å†…çƒ­é—¨æ—…æ¸¸ç›®çš„åœ°"
- "å›½å†…" â†’ "å›½å†…ç»å…¸æ—…æ¸¸è·¯çº¿"
- "æ•…å®«æ€ä¹ˆå»" â†’ "æ•…å®«æ€ä¹ˆå»"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰
- "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„" â†’ "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰

è¯·è¾“å‡ºæœ€ç»ˆæŸ¥è¯¢ï¼ˆå¦‚æœä¸éœ€è¦é‡å†™å°±è¿”å›åŸæŸ¥è¯¢ï¼‰:""",
            input_variables=["query"]
        )

        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query).strip()

        # è®°å½•é‡å†™ç»“æœ
        if response != query:
            logger.info(f"æŸ¥è¯¢å·²é‡å†™: '{query}' â†’ '{response}'")
        else:
            logger.info(f"æŸ¥è¯¢æ— éœ€é‡å†™: '{query}'")

        return response



    def query_router(self, query: str) -> str:
        """
        æŸ¥è¯¢è·¯ç”± - æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            è·¯ç”±ç±»å‹ ('list', 'detail', 'general')
        """
        prompt = ChatPromptTemplate.from_template("""
æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œå°†å…¶åˆ†ç±»ä¸ºä»¥ä¸‹ä¸‰ç§ç±»å‹ä¹‹ä¸€ï¼š

1. 'list' - ç”¨æˆ·æƒ³è¦è·å–æ™¯ç‚¹åˆ—è¡¨æˆ–æ¨èï¼Œåªéœ€è¦æ™¯ç‚¹åç§°
   ä¾‹å¦‚ï¼šæ¨èå‡ ä¸ªæ™¯ç‚¹ã€åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„ã€ç»™æˆ‘3ä¸ªå¿…å»çš„åœ°æ–¹

2. 'detail' - ç”¨æˆ·æƒ³è¦å…·ä½“çš„æ—…æ¸¸ä¿¡æ¯æˆ–è¯¦ç»†æŒ‡å—
   ä¾‹å¦‚ï¼šæ•…å®«æ€ä¹ˆå»ã€é—¨ç¥¨å¤šå°‘é’±ã€å¼€æ”¾æ—¶é—´ã€æ—…æ¸¸æ”»ç•¥

3. 'general' - å…¶ä»–ä¸€èˆ¬æ€§é—®é¢˜
   ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æ–‡åŒ–æ—…æ¸¸ã€æ—…æ¸¸æ³¨æ„äº‹é¡¹ã€æœ€ä½³æ—…æ¸¸å­£èŠ‚

è¯·åªè¿”å›åˆ†ç±»ç»“æœï¼šlistã€detail æˆ– general

ç”¨æˆ·é—®é¢˜: {query}

åˆ†ç±»ç»“æœ:""")

        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        result = chain.invoke(query).strip().lower()

        # ç¡®ä¿è¿”å›æœ‰æ•ˆçš„è·¯ç”±ç±»å‹
        if result in ['list', 'detail', 'general']:
            return result
        else:
            return 'general'  # é»˜è®¤ç±»å‹

    def generate_list_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”Ÿæˆåˆ—è¡¨å¼å›ç­” - é€‚ç”¨äºæ¨èç±»æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            åˆ—è¡¨å¼å›ç­”
        """
        if not context_docs:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ—…æ¸¸æ™¯ç‚¹ä¿¡æ¯ã€‚"

        # æå–åœ°ç‚¹åç§°
        location_names = []
        for doc in context_docs:
            location_name = doc.metadata.get('location_name', 'æœªçŸ¥åœ°ç‚¹')
            if location_name not in location_names:
                location_names.append(location_name)

        # æ„å»ºç®€æ´çš„åˆ—è¡¨å›ç­”
        if len(location_names) == 1:
            return f"ä¸ºæ‚¨æ¨èï¼š{location_names[0]}"
        elif len(location_names) <= 3:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹æ™¯ç‚¹ï¼š\n" + "\n".join([f"{i+1}. {name}" for i, name in enumerate(location_names)])
        else:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹æ™¯ç‚¹ï¼š\n" + "\n".join([f"{i+1}. {name}" for i, name in enumerate(location_names[:3])]) + f"\n\nè¿˜æœ‰å…¶ä»– {len(location_names)-3} ä¸ªæ™¯ç‚¹å¯ä¾›é€‰æ‹©ã€‚"

    def generate_basic_answer_stream(self, query: str, context_docs: List[Document]):
        """
        ç”ŸæˆåŸºç¡€å›ç­” - æµå¼è¾“å‡º

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Yields:
            ç”Ÿæˆçš„å›ç­”ç‰‡æ®µ
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸é¡¾é—®ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ—…æ¸¸ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³æ—…æ¸¸ä¿¡æ¯:
{context}

è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        for chunk in chain.stream(query):
            yield chunk

    def generate_detailed_guide_answer_stream(self, query: str, context_docs: List[Document]):
        """
        ç”Ÿæˆè¯¦ç»†æ—…æ¸¸æŒ‡å—å›ç­” - æµå¼è¾“å‡º

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Yields:
            è¯¦ç»†æ—…æ¸¸æŒ‡å—å›ç­”ç‰‡æ®µ
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸è§„åˆ’å¸ˆã€‚è¯·æ ¹æ®æ—…æ¸¸ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„æ—…æ¸¸æŒ‡å—ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³æ—…æ¸¸ä¿¡æ¯:
{context}

è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

## ğŸï¸ æ™¯ç‚¹ä»‹ç»
[ç®€è¦ä»‹ç»æ™¯ç‚¹ç‰¹è‰²å’Œäº®ç‚¹]

## ğŸ“ åŸºæœ¬ä¿¡æ¯
[åœ°å€ã€å¼€æ”¾æ—¶é—´ã€é—¨ç¥¨ä»·æ ¼ã€è”ç³»æ–¹å¼ç­‰]

## ğŸš— äº¤é€šæŒ‡å—
[å¦‚ä½•åˆ°è¾¾ï¼ŒåŒ…æ‹¬å…¬å…±äº¤é€šå’Œè‡ªé©¾è·¯çº¿]

## ğŸ’¡ æ¸¸è§ˆå»ºè®®
[æœ€ä½³æ¸¸è§ˆæ—¶é—´ã€æ¨èè·¯çº¿ã€æ³¨æ„äº‹é¡¹ç­‰]

æ³¨æ„ï¼š
- æ ¹æ®å®é™…å†…å®¹çµæ´»è°ƒæ•´ç»“æ„
- ä¸è¦å¼ºè¡Œå¡«å……æ— å…³å†…å®¹
- é‡ç‚¹çªå‡ºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        for chunk in chain.stream(query):
            yield chunk

    def _build_context(self, docs: List[Document], max_length: int = 2000) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²

        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦

        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not docs:
            return "æš‚æ— ç›¸å…³æ—…æ¸¸ä¿¡æ¯ã€‚"

        context_parts = []
        current_length = 0

        for i, doc in enumerate(docs, 1):
            # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
            metadata_info = f"ã€æ—…æ¸¸ä¿¡æ¯ {i}ã€‘"
            if 'location_name' in doc.metadata:
                metadata_info += f" {doc.metadata['location_name']}"
            if 'category' in doc.metadata:
                metadata_info += f" | åˆ†ç±»: {doc.metadata['category']}"
            if 'city' in doc.metadata:
                metadata_info += f" | åŸå¸‚: {doc.metadata['city']}"
            if 'price_level' in doc.metadata:
                metadata_info += f" | ä»·æ ¼: {doc.metadata['price_level']}"

            # æ„å»ºæ–‡æ¡£æ–‡æœ¬
            doc_text = f"{metadata_info}\n{doc.page_content}\n"

            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(doc_text) > max_length:
                break

            context_parts.append(doc_text)
            current_length += len(doc_text)

        return "\n" + "="*50 + "\n".join(context_parts)